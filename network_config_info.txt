        self.dense1 = keras.layers.Dense(fc1_dims, activation="selu")
        # ^Add a first densely connected layer, I think this is the input layer, though it might actually not be
        self.dense2 = keras.layers.Dense(fc2_dims, activation="exponential")
        self.dense3 = keras.layers.Dense(fc3_dims, activation="selu")
        self.dense4 = keras.layers.Dense(fc3_dims, activation="selu")
        self.dense5 = keras.layers.Dense(fc3_dims, activation="selu")
        self.dense6 = keras.layers.Dense(fc3_dims, activation="selu")
        # ^add a second dense layer, this should be a hidden layer within the network
        self.V = keras.layers.Dense(1, activation=None)
        # This looks to be a "value" layer that compresses the output of the network to a single value
        #We use the sigmoid function to force all of the values contained within actions to be between 0 and 1.
        self.A = keras.layers.Dense(n_actions, activation='softplus')

    def __init__(self, lr=1e-7, gamma=.55, n_actions=8, epsilon=.99, batch_size=64,
                    input_dims=[1228], epsilon_dec=1e-4, epsilon_min=1e-3,
                    mem_size=100000, fname='dueling_dqn.h5', fc1_dims=1228,
                    fc2_dims=1228, fc3_dims=1228, fc4_dims=1024, fc5_dims=512, fc6_dims=256, replace=500):