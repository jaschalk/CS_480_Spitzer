    def __init__(self, lr=1.5e-3, gamma=.6, n_actions=8, epsilon=.9, batch_size=64,
                    input_dims=[1228], epsilon_dec=1e-3, epsilon_min=1e-3,
                    mem_size=100000, fname='dueling_dqn.h5', fc1_dims=1228,
                    fc2_dims=1228, fc3_dims=512, replace=500):

        self.dense1 = keras.layers.Dense(fc1_dims, activation="exponential")
        # ^Add a first densely connected layer, I think this is the input layer, though it might actually not be
        self.dense2 = keras.layers.Dense(fc2_dims, activation="exponential")
        self.dense3 = keras.layers.Dense(fc3_dims, activation="relu")
        # ^add a second dense layer, this should be a hidden layer within the network
        self.V = keras.layers.Dense(1, activation=None)
        # This looks to be a "value" layer that compresses the output of the network to a single value
        #We use the sigmoid function to force all of the values contained within actions to be between 0 and 1.
        self.A = keras.layers.Dense(n_actions, activation='softmax')